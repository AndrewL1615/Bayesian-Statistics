---
title: "HW3"
author: "Andrew Liang"
date: "9/21/2020"
output: pdf_document
---

# 4.2

**a)**
```{r}
#sampling a
ya <- c(12,9,12,14,13,13,15,8,15,6)
na <- length(ya)
sya <- sum(ya)

a1 <- 120; a2 <- 10 #prior parameters 

yb <- c(11,11,10,9,9,8,7,10,6,8,8,9,7)
nb <- length(yb)
syb <- sum(yb)

b1 <- 12; b2 <- 1

#Monte Carlo sampling
set.seed(1651)

thetaa.mc <- rgamma(10000, a1 + sya, a2 + na) #using 10000 samplings for each theta
thetab.mc <- rgamma(10000, b1 + syb, b2 + nb)

mean(thetab.mc < thetaa.mc)
```

**b)**
```{r}
n0 <- seq(1:100)

set.seed(1651)
postprob <- c() # for inputting probability value for each iteration

#calculating posterior prob. theta b < theta a for values of n0
for(i in 1:length(n0)){
  thetab.mc <- rgamma(10000, b1*i + syb, i + nb)
  postprob[i] <- mean(thetab.mc < thetaa.mc)
}

print(postprob)
plot(n0, postprob, type = "l", main = "Posterior Prob. Theta B < Theta A", xlab = "n0", ylab = "Probability")
```

From the output above, we can see all the values of $Pr(\theta_{B} < \theta_{A}|y_{A},y_{B})$ for each $n_{0} = {1,2,\cdots,100}$ and the plot associated with it. We can see that although there is an obvious negative effect on the posterior probability for an increasing $n_{0}$, it is arguable that conclusions about event $\theta_{B} <\theta_{A}$ is not very sensitive from the prior distribution of $\theta_{B}$. Even if $n_{0} = 100$, $Pr(\theta_{B} < \theta_{A}|y_{A},y_{B})$ is still greater than 0.6.

**c)**
```{r}
thetaa.mc <- rgamma(10000, a1 + sya, a2 + na)
thetab.mc <- rgamma(10000, b1 + syb, b2 + nb)

#posterior predictive sampling
set.seed(1651)

ya.mc <- rpois(10000, thetaa.mc)
yb.mc <- rpois(10000, thetab.mc)

mean(yb.mc < ya.mc)
```
Thus, $Pr(\tilde{Y_{B}} < \tilde{Y_{A}}|y_{A},y_{B})$ = 0.6996

```{r}
set.seed(1651)
postpred <- c() 

#calculating posterior prob. theta b < theta a for values of n0
for(i in 1:length(n0)){
  thetab.mc <- rgamma(10000, b1*i + syb, i + nb)
  newyb.mc <- rpois(10000, thetab.mc)
  postpred[i] <- mean(newyb.mc < ya.mc)
}

print(postpred)
plot(n0, postpred, type = "l", main = "Posterior Prob. Y tilda B < Y tilda A", xlab = "n0", ylab = "Probability")
```
From the output above, we can see all the values of $Pr(\tilde{Y_{B}} < \tilde{Y_{A}}|y_{A},y_{B})$ for $n_{0} = 1,2,\cdots,100$, as well as a plot of the two. Comparing this to the result in part b), we can argue that the conclusions of $\tilde{Y_{B}} < \tilde{Y_{A}}$ are more sensitive. For even $n_{0} = 1$, we can see that the probability starts initially at 0.7, and goes below 0.5 as $n_{0}$ tends to 100. We also note that the pattern of probabilities are much more sensitive/volatile as $n_{0}$ increases, relative to that of part b).


# 4.3 

**a)**
```{r}
ta.mc <- c() #sample statistic of interest
set.seed(1651)

for(s in 1:1000){
  thetaa.mc <- rgamma(1, a1 + sya, a2 + na)
  ya.mc <- rpois(na,thetaa.mc)
  ta.mc[s] <- mean(ya.mc)/sd(ya.mc) 
}
ta.obs <- mean(ya)/sd(ya)

hist(ta.mc, main = "Histogram of t-statistic for sample A", xlab = "t")
abline(v = ta.obs, col = "red", lwd = 2)

#calculating absolute mean difference for all t.mc iterations
for(i in 1:1000){
  diff <- abs(ta.mc[i] - ta.obs)
}
mean(diff)
```
From the histogram, we can see that the mode of the statistic through the Monte Carlo samples is pretty close to the observed value of the statistic, with an mean absolute difference of about 0.26. Thus, the Poisson is a pretty good fit for the test statistic based on sample A.

**b)**
```{r}
tb.mc <- c()
set.seed(1651)

for(s in 1:1000){
  thetab.mc <- rgamma(1, b1 + syb, b2 + nb)
  yb.mc <- rpois(nb,thetab.mc)
  tb.mc[s] <- mean(yb.mc)/sd(yb.mc) 
}
tb.obs <- mean(yb)/sd(yb)

hist(tb.mc, main = "Histogram of t-statistic for sample B", xlab = "t")
abline(v = tb.obs, col = "blue", lwd = 2)

for(i in 1:1000){
  diff <- abs(tb.mc[i] - tb.obs)
}
mean(diff)
```

From the histogram, we can see that the mode of the statistic through the Monte Carlo samples is much further way from the observed value of the statistic, with an mean absolute difference of about 2.67. Thus, the Poisson is not a good fit for the test statistic based on sample B.